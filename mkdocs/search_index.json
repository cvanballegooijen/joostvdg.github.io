{
    "docs": [
        {
            "location": "/", 
            "text": "Flusso CI/CD Knowledge Docs!\n\n\nCI/CD\n stands for \nContinuous Integration\n \n \nContinuous Delivery\n.\n\n\nThis is a collection of docs for sharing Knowledge on CI/CD topics. We try to start things from customers' perspective in order to re-use all topics for multiple customers of Flusso. The main customer regarding CI/CD is ABN-ARMO bank. This acts as a starting point for this knowledge base. In the (near) future we might replace the contents of the wiki pages at Flusso with these way of documenting/knowlegde sharing.\n\n\nOK let's start with \"Documenting as code\". Have fun! ;-)\nYes, we will! :-)\nWhen? Now!\n\n\nContinuous Integration\n\n\nA good definition can be found here: \nhttp://www.martinfowler.com/articles/continuousIntegration.html\n\n\nContinuous Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible. Many teams find that this approach leads to significantly reduced integration problems and allows a team to develop cohesive software more rapidly.\"\n\n\nContinuous Delivery\n\n\nContinous Delivery/deployment is the next step in getting yr software changes at the desired server in order to let your clients take a look at it.\nThis article provides a good example of it: \nhttp://www.martinfowler.com/articles/continuousIntegration.html\n\n\nTo do Continuous Integration you need multiple environments, one to run commit tests, one or more to run secondary tests. Since you are moving executables between these environments multiple times a day, you'll want to do this automatically. So it's important to have scripts that will allow you to deploy the application into any environment easily.", 
            "title": "Home"
        }, 
        {
            "location": "/#flusso-cicd-knowledge-docs", 
            "text": "CI/CD  stands for  Continuous Integration     Continuous Delivery .  This is a collection of docs for sharing Knowledge on CI/CD topics. We try to start things from customers' perspective in order to re-use all topics for multiple customers of Flusso. The main customer regarding CI/CD is ABN-ARMO bank. This acts as a starting point for this knowledge base. In the (near) future we might replace the contents of the wiki pages at Flusso with these way of documenting/knowlegde sharing.  OK let's start with \"Documenting as code\". Have fun! ;-)\nYes, we will! :-)\nWhen? Now!", 
            "title": "Flusso CI/CD Knowledge Docs!"
        }, 
        {
            "location": "/#continuous-integration", 
            "text": "A good definition can be found here:  http://www.martinfowler.com/articles/continuousIntegration.html  Continuous Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible. Many teams find that this approach leads to significantly reduced integration problems and allows a team to develop cohesive software more rapidly.\"", 
            "title": "Continuous Integration"
        }, 
        {
            "location": "/#continuous-delivery", 
            "text": "Continous Delivery/deployment is the next step in getting yr software changes at the desired server in order to let your clients take a look at it.\nThis article provides a good example of it:  http://www.martinfowler.com/articles/continuousIntegration.html  To do Continuous Integration you need multiple environments, one to run commit tests, one or more to run secondary tests. Since you are moving executables between these environments multiple times a day, you'll want to do this automatically. So it's important to have scripts that will allow you to deploy the application into any environment easily.", 
            "title": "Continuous Delivery"
        }, 
        {
            "location": "/jenkins/", 
            "text": "Jenkins\n\n\nCloudbees Study Guide\n\n\nBase configuration\n\n\nabc\n\n\nTuning\n\n\nPlease read the following articles from Cloudbees:\n\n\n\n\nPrepare-Jenkins-for-support\n\n\ntuning-jenkins-gc-responsiveness-and-stability\n\n\nAfter-moving-a-job-symlinks-for-folders-became-actual-folders\n\n\nHow-to-disable-the-weather-column-to-resolve-instance-slowness\n\n\nAccessing-graphs-on-a-Build-History-page-can-cause-Jenkins-to-become-unresponsive\n\n\nAutoBrowser-Feature-Can-Cause-Performance-Issues\n\n\nDisk-Space-Issue-after-upgrading-Branch-API-plugin\n\n\nJVM-Memory-settings-best-practice\n\n\n\n\nPipeline as code\n\n\n\n\nThe default interaction model with Jenkins, historically, has been very web UI driven, requiring users to manually create jobs, then manually fill in the details through a web browser. This requires additional effort to create and manage jobs to test and build multiple projects, it also keeps the configuration of a job to build/test/deploy separate from the actual code being built/tested/deployed. This prevents users from applying their existing CI/CD best practices to the job configurations themselves.\n\n\nWith the introduction of the Pipeline plugin, users now can implement a project\u2019s entire build/test/deploy pipeline in a Jenkinsfile and store that alongside their code, treating their pipeline as another piece of code checked into source control.\n\n\n\n\nWe will dive into several things that come into play when writing Jenkins pipelines.\n\n\n\n\nKind of Pipeline jobs\n\n\nInfo about Pipeline DSL (a groovy DSL)\n\n\nReuse pipeline DSL scripts\n\n\nThings to keep in mind\n\n\nDo's and Don't\n\n\n\n\nResources\n\n\n\n\nPipeline Steps\n\n\nPipeline Solution\n\n\nPipeline as Code\n\n\nDzone RefCard\n\n\n\n\nType of pipeline jobs\n\n\n\n\nPipeline (inline)\n\n\nPipeline (from SCM)\n\n\nMulti-Branch Pipeline\n\n\nGitHub Organization\n\n\nBitBucket Team/Project\n\n\n\n\n\n\nDanger\n\n\nWhen using the \nstash function\n keep in mind that the copying goes from where you are now to the master.\nWhen you unstash, it will copy the files from the master to where you are building.\n\n\nWhen your pipeline runs on a node and you stash and then unstash, it will copy the files from the node to the master and then back to the node.\nThis can have a severe penalty on the performance of your pipeline when you are copying over a network.\n\n\n\n\nAPI\n\n\nJenkins has an extensive \nAPI\n allowing you to retrieve a lot of information from the server.\n\n\nPlugin\n\n\nFor this way you of course have to know how to write a plugin.\nThere are some usefull resources to get started:\n* \nhttps://github.com/joostvdg/hello-world-jenkins-pipeline-plugin\n\n* \nhttps://wiki.jenkins-ci.org/display/JENKINS/Plugin+tutorial\n\n* \nhttps://jenkins.io/blog/2016/05/25/update-plugin-for-pipeline/\n\n\nDo's and Don't\n\n\nAside from the \nDo's and Don'ts\n from Cloudbees, there are some we want to share.\n\n\nThis changes the requirement for the component identifier property, as a job may only match a single group and a job listing in a group can only match a single. Thus the easiest way to make sure everything will stay unique (template names probably don\u2019t), is to make the component identifier property unique per file - let it use the name of the project.", 
            "title": "Jenkins"
        }, 
        {
            "location": "/jenkins/#jenkins", 
            "text": "Cloudbees Study Guide", 
            "title": "Jenkins"
        }, 
        {
            "location": "/jenkins/#base-configuration", 
            "text": "abc", 
            "title": "Base configuration"
        }, 
        {
            "location": "/jenkins/#tuning", 
            "text": "Please read the following articles from Cloudbees:   Prepare-Jenkins-for-support  tuning-jenkins-gc-responsiveness-and-stability  After-moving-a-job-symlinks-for-folders-became-actual-folders  How-to-disable-the-weather-column-to-resolve-instance-slowness  Accessing-graphs-on-a-Build-History-page-can-cause-Jenkins-to-become-unresponsive  AutoBrowser-Feature-Can-Cause-Performance-Issues  Disk-Space-Issue-after-upgrading-Branch-API-plugin  JVM-Memory-settings-best-practice", 
            "title": "Tuning"
        }, 
        {
            "location": "/jenkins/#pipeline-as-code", 
            "text": "The default interaction model with Jenkins, historically, has been very web UI driven, requiring users to manually create jobs, then manually fill in the details through a web browser. This requires additional effort to create and manage jobs to test and build multiple projects, it also keeps the configuration of a job to build/test/deploy separate from the actual code being built/tested/deployed. This prevents users from applying their existing CI/CD best practices to the job configurations themselves.  With the introduction of the Pipeline plugin, users now can implement a project\u2019s entire build/test/deploy pipeline in a Jenkinsfile and store that alongside their code, treating their pipeline as another piece of code checked into source control.   We will dive into several things that come into play when writing Jenkins pipelines.   Kind of Pipeline jobs  Info about Pipeline DSL (a groovy DSL)  Reuse pipeline DSL scripts  Things to keep in mind  Do's and Don't", 
            "title": "Pipeline as code"
        }, 
        {
            "location": "/jenkins/#resources", 
            "text": "Pipeline Steps  Pipeline Solution  Pipeline as Code  Dzone RefCard", 
            "title": "Resources"
        }, 
        {
            "location": "/jenkins/#type-of-pipeline-jobs", 
            "text": "Pipeline (inline)  Pipeline (from SCM)  Multi-Branch Pipeline  GitHub Organization  BitBucket Team/Project    Danger  When using the  stash function  keep in mind that the copying goes from where you are now to the master.\nWhen you unstash, it will copy the files from the master to where you are building.  When your pipeline runs on a node and you stash and then unstash, it will copy the files from the node to the master and then back to the node.\nThis can have a severe penalty on the performance of your pipeline when you are copying over a network.", 
            "title": "Type of pipeline jobs"
        }, 
        {
            "location": "/jenkins/#api", 
            "text": "Jenkins has an extensive  API  allowing you to retrieve a lot of information from the server.", 
            "title": "API"
        }, 
        {
            "location": "/jenkins/#plugin", 
            "text": "For this way you of course have to know how to write a plugin.\nThere are some usefull resources to get started:\n*  https://github.com/joostvdg/hello-world-jenkins-pipeline-plugin \n*  https://wiki.jenkins-ci.org/display/JENKINS/Plugin+tutorial \n*  https://jenkins.io/blog/2016/05/25/update-plugin-for-pipeline/", 
            "title": "Plugin"
        }, 
        {
            "location": "/jenkins/#dos-and-dont", 
            "text": "Aside from the  Do's and Don'ts  from Cloudbees, there are some we want to share.  This changes the requirement for the component identifier property, as a job may only match a single group and a job listing in a group can only match a single. Thus the easiest way to make sure everything will stay unique (template names probably don\u2019t), is to make the component identifier property unique per file - let it use the name of the project.", 
            "title": "Do's and Don't"
        }, 
        {
            "location": "/jenkins-jobs/jobdsl/", 
            "text": "Jenkins Job DSL\n\n\nJenkins is a wonderful system for managing builds, and people love using its UI to configure jobs. Unfortunately, as the number of jobs grows, maintaining them becomes tedious, and the paradigm of using a UI falls apart. Additionally, the common pattern in this situation is to copy jobs to create new ones, these \"children\" have a habit of diverging from their original \"template\" and consequently it becomes difficult to maintain consistency between these jobs.\n\n\nThe Jenkins job-dsl-plugin attempts to solve this problem by allowing jobs to be defined with the absolute minimum necessary in a programmatic form, with the help of templates that are synced with the generated jobs. The goal is for your project to be able to define all the jobs they want to be related to their project, declaring their intent for the jobs, leaving the common stuff up to a template that were defined earlier or hidden behind the DSL.\n\n\nPipeline with folder example\n\n\nimport\n \nhudson.model.*\n\n\nimport\n \njenkins.model.*\n\n\n\ndef\n \ndslExamplesFolder\n \n=\n \nDSL-Examples\n\n\ndef\n \ngitLabCredentialsId\n \n=\n \njoost-flusso-gitlab-ssh\n\n\ndef\n \ngitLabUrl\n \n=\n \ngit@gitlab.flusso.nl\n\n\ndef\n \ngitLabNamespace\n \n=\n \nkeep\n\n\ndef\n \ngitLabProject\n \n=\n \nkeep-api\n\n\n\n\nif\n(!\njenkins\n.\nmodel\n.\nJenkins\n.\ninstance\n.\ngetItem\n(\ndslExamplesFolder\n))\n \n{\n\n    \n//folder doesn\nt exist because item doesn\nt exist in runtime\n\n    \n//Therefore, create the folder.\n\n    \nfolder\n(\ndslExamplesFolder\n)\n \n{\n\n        \ndisplayName\n(\nDSL Examples\n)\n\n        \ndescription\n(\nFolder for job dsl examples\n)\n\n    \n}\n\n\n}\n\n\n\ncreateMultibranchPipelineJob\n(\ngitLabCredentialsId\n,\n \ngitLabUrl\n,\n \ndslExamplesFolder\n,\n \nkeep\n,\n \nkeep-api\n)\n\n\ncreateMultibranchPipelineJob\n(\ngitLabCredentialsId\n,\n \ngitLabUrl\n,\n \ndslExamplesFolder\n,\n \nkeep\n,\n \nkeep-backend-spring\n)\n\n\ncreateMultibranchPipelineJob\n(\ngitLabCredentialsId\n,\n \ngitLabUrl\n,\n \ndslExamplesFolder\n,\n \nkeep\n,\n \nkeep-frontend\n)\n\n\n\ndef\n \ncreateMultibranchPipelineJob\n(\ndef\n \ngitLabCredentialsId\n,\n \ndef\n \ngitLabUrl\n,\n \ndef\n \nfolder\n,\n \ndef\n \ngitNamespace\n,\n \ndef\n \nproject\n)\n \n{\n\n    \nmultibranchPipelineJob\n(\n${folder}/${project}-mb\n)\n \n{\n\n        \nbranchSources\n \n{\n\n            \ngit\n \n{\n\n                \nremote\n(\n${gitLabUrl}:${gitNamespace}/${project}.git\n)\n\n                \ncredentialsId\n(\ngitLabCredentialsId\n)\n\n            \n}\n\n        \n}\n\n        \norphanedItemStrategy\n \n{\n\n            \ndiscardOldItems\n \n{\n\n                \nnumToKeep\n(\n20\n)\n\n            \n}\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\nFreestyle maven job\n\n\ndef\n \nproject\n \n=\n \nquidryan/aws-sdk-test\n\n\ndef\n \nbranchApi\n \n=\n \nnew\n \nURL\n(\nhttps://api.github.com/repos/${project}/branches\n)\n\n\ndef\n \nbranches\n \n=\n \nnew\n \ngroovy\n.\njson\n.\nJsonSlurper\n().\nparse\n(\nbranchApi\n.\nnewReader\n())\n\n\nbranches\n.\neach\n \n{\n\n    \ndef\n \nbranchName\n \n=\n \nit\n.\nname\n\n    \ndef\n \njobName\n \n=\n \n${project}-${branchName}\n.\nreplaceAll\n(\n/\n,\n-\n)\n\n    \njob\n(\njobName\n)\n \n{\n\n        \nscm\n \n{\n\n            \ngit\n(\ngit://github.com/${project}.git\n,\n \nbranchName\n)\n\n        \n}\n\n        \nsteps\n \n{\n\n            \nmaven\n(\ntest -Dproject.name=${project}/${branchName}\n)\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\nResources\n\n\n\n\nTutorial\n\n\nLive Playground\n\n\nMain DSL Commands\n\n\nAPI Viewer\n\n\n\n\nOther References\n\n\n\n\nTalks and Blogs\n\n\nUser Power Movies\n\n\nDZone article\n\n\nTesting DSL Scripts", 
            "title": "JobDSL"
        }, 
        {
            "location": "/jenkins-jobs/jobdsl/#jenkins-job-dsl", 
            "text": "Jenkins is a wonderful system for managing builds, and people love using its UI to configure jobs. Unfortunately, as the number of jobs grows, maintaining them becomes tedious, and the paradigm of using a UI falls apart. Additionally, the common pattern in this situation is to copy jobs to create new ones, these \"children\" have a habit of diverging from their original \"template\" and consequently it becomes difficult to maintain consistency between these jobs.  The Jenkins job-dsl-plugin attempts to solve this problem by allowing jobs to be defined with the absolute minimum necessary in a programmatic form, with the help of templates that are synced with the generated jobs. The goal is for your project to be able to define all the jobs they want to be related to their project, declaring their intent for the jobs, leaving the common stuff up to a template that were defined earlier or hidden behind the DSL.", 
            "title": "Jenkins Job DSL"
        }, 
        {
            "location": "/jenkins-jobs/jobdsl/#pipeline-with-folder-example", 
            "text": "import   hudson.model.*  import   jenkins.model.*  def   dslExamplesFolder   =   DSL-Examples  def   gitLabCredentialsId   =   joost-flusso-gitlab-ssh  def   gitLabUrl   =   git@gitlab.flusso.nl  def   gitLabNamespace   =   keep  def   gitLabProject   =   keep-api  if (! jenkins . model . Jenkins . instance . getItem ( dslExamplesFolder ))   { \n     //folder doesn t exist because item doesn t exist in runtime \n     //Therefore, create the folder. \n     folder ( dslExamplesFolder )   { \n         displayName ( DSL Examples ) \n         description ( Folder for job dsl examples ) \n     }  }  createMultibranchPipelineJob ( gitLabCredentialsId ,   gitLabUrl ,   dslExamplesFolder ,   keep ,   keep-api )  createMultibranchPipelineJob ( gitLabCredentialsId ,   gitLabUrl ,   dslExamplesFolder ,   keep ,   keep-backend-spring )  createMultibranchPipelineJob ( gitLabCredentialsId ,   gitLabUrl ,   dslExamplesFolder ,   keep ,   keep-frontend )  def   createMultibranchPipelineJob ( def   gitLabCredentialsId ,   def   gitLabUrl ,   def   folder ,   def   gitNamespace ,   def   project )   { \n     multibranchPipelineJob ( ${folder}/${project}-mb )   { \n         branchSources   { \n             git   { \n                 remote ( ${gitLabUrl}:${gitNamespace}/${project}.git ) \n                 credentialsId ( gitLabCredentialsId ) \n             } \n         } \n         orphanedItemStrategy   { \n             discardOldItems   { \n                 numToKeep ( 20 ) \n             } \n         } \n     }  }", 
            "title": "Pipeline with folder example"
        }, 
        {
            "location": "/jenkins-jobs/jobdsl/#freestyle-maven-job", 
            "text": "def   project   =   quidryan/aws-sdk-test  def   branchApi   =   new   URL ( https://api.github.com/repos/${project}/branches )  def   branches   =   new   groovy . json . JsonSlurper (). parse ( branchApi . newReader ())  branches . each   { \n     def   branchName   =   it . name \n     def   jobName   =   ${project}-${branchName} . replaceAll ( / , - ) \n     job ( jobName )   { \n         scm   { \n             git ( git://github.com/${project}.git ,   branchName ) \n         } \n         steps   { \n             maven ( test -Dproject.name=${project}/${branchName} ) \n         } \n     }  }", 
            "title": "Freestyle maven job"
        }, 
        {
            "location": "/jenkins-jobs/jobdsl/#resources", 
            "text": "Tutorial  Live Playground  Main DSL Commands  API Viewer", 
            "title": "Resources"
        }, 
        {
            "location": "/jenkins-jobs/jobdsl/#other-references", 
            "text": "Talks and Blogs  User Power Movies  DZone article  Testing DSL Scripts", 
            "title": "Other References"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/", 
            "text": "Jenkins Job Builder\n\n\nThe configuration setup of Jenkins Job Builder is composed of two main categories. Basic configuration and job configuration. Job configuration can be further split into several sub categories.\n\n\nBasic Configuration\n\n\nIn the basic configuration you will have to specify how the Jenkins Job Builder CLI can connect to the Jenkins instance you want to configure and how it should act.\n\n\nTo use such a configuration file, you add --conf \n to the CLI command.\n\n\nExample:\n\nlocalhost.ini\n[job_builder]\nignore_cache=True\nkeep_descriptions=False\ninclude_path=.:scripts:~/git/\nrecursive=False\nexclude=.*:manual:./development\nallow_duplicates=False\n\n[jenkins]\n#user=jenkins\n#password=\nurl=http://localhost:8080/\n\n\n\nFor more information see \nhttp://docs.openstack.org/infra/jenkins-job-builder/installation.html\n.\n\n\nJob Configuration\n\n\nThe configuration for configuring the jobs consists of several distinct parts which can all be in the same file or can be distributed in their own respected files.\n\n\nThese different parts can also be split into two different categories, those that are strictly linked within the configuration - via template matching - and those that are separate.\n\n\nSeparate:\n* Macro\u2019s\n* Global defaults\n* Job configuration defaults\n* External configuration files\n\n\nLinked:\n* Templates\n* Groups\n* Projects\n* Job definitions\n\n\n\n\nHere\u2019s a schematic representation on how they are linked.\nExampe in YAML config:\n\n-\n \njob-template\n:\n\n    \nname\n:\n \n{name}-{configComponentId}-ci\n\n    \ndescription\n:\n \nCI\n \nJob\n \nof\n \n{configComponentId}\n\n    \n:\n \n*config_job_defaults\n\n    \nbuilders\n:\n\n        \n-\n \nshell\n:\n \njenkins-jobs\n \ntest\n \n-r\n \nglobal/:definitions/\n \n-o\n \ncompiled/\n\n\n-\n \njob-template\n:\n\n    \nname\n:\n \n{name}-{configComponentId}-execute\n\n    \ndescription\n:\n \nExecutor\n \nJob\n \nof\n \n{configComponentId}\n\n    \n:\n \n*config_job_defaults\n\n    \nbuilders\n:\n\n        \n-\n \nshell\n:\n \njenkins-jobs\n \n--conf\n \nconfiguration/localhost.ini\n \nupdate\n \ndefinitions/\n\n\n-\n \njob-group\n:\n\n    \nname\n:\n \n{name}-config\n\n    \ngitlab-user\n:\n \njvandergriendt\n\n    \njobs\n:\n\n        \n-\n \n{name}-{configComponentId}-ci\n:\n\n        \n-\n \n{name}-{configComponentId}-execute\n:\n\n\n\n-\n \nproject\n:\n\n    \nname\n:\n \nRnD-Config\n\n    \njobs\n:\n\n        \n-\n \n{name}-config\n:\n\n            \nconfigComponentId\n:\n \nJenkinsJobDefinitions\n\n\n\n\nThe above will result in the following jobs:\nRnD-Config-JenkinsJobDefinitions-ci\nRnD-Config-JenkinsJobDefinitions-execute\n\n\nMacro\u2019s\n\n\nMacro\u2019s are what the name implies, a group of related commands which can be invoked by the group. In Jenkins Job Builder this means you can define specific configurations for a component type (e.g. builders, paramters, publishes etc).\n\n\nA component has a name and a macro name. In general the component name is plural and the macro name is singular. As can be seen in the examples below.\n\n\nHere\u2019s an example:\n\n# The \nadd\n macro takes a \nnumber\n parameter and will creates a\n\n\n# job which prints \nAdding \n followed by the \nnumber\n parameter:\n\n\n-\n \nbuilder\n:\n\n    \nname\n:\n \nadd\n\n    \nbuilders\n:\n\n     \n-\n \nshell\n:\n \necho\n \nAdding\n \n{number}\n\n\n\n# A specialized macro \naddtwo\n reusing the \nadd\n macro but with\n\n\n# a \nnumber\n parameter hardcoded to \ntwo\n:\n\n\n-\n \nbuilder\n:\n\n    \nname\n:\n \naddtwo\n\n    \nbuilders\n:\n\n     \n-\n \nadd\n:\n\n        \nnumber\n:\n \ntwo\n\n\n\n# Glue to have Jenkins Job Builder to expand this YAML example:\n\n\n-\n \njob\n:\n\n    \nname\n:\n \ntestingjob\n\n    \nbuilders\n:\n\n     \n# The specialized macro:\n\n     \n-\n \naddtwo\n\n     \n# Generic macro call with a parameter\n\n     \n-\n \nadd\n:\n\n        \nnumber\n:\n \nZERO\n\n     \n# Generic macro called without a parameter. Never do this!\n\n     \n# See below for the resulting wrong output :(\n\n     \n-\n \nadd\n\n\n\n\n\n\nTo expand the schematic representation, you will get the following.\n\n\n-\n \nbuilder\n:\n\n    \nname\n:\n \ntest\n\n    \nbuilders\n:\n\n     \n-\n \nshell\n:\n \njenkins-jobs\n \ntest\n \n-r\n \nglobal/:definitions/\n \n-o\n \ncompiled/\n\n\n\n-\n \nbuilder\n:\n\n    \nname\n:\n \nupdate\n\n    \nbuilders\n:\n\n     \n-\n \nshell\n:\n \njenkins-jobs\n \n--conf\n \nconfig.ini\n \nupdate\n \n-r\n \nglobal/:definitions/\n\n\n\n-\n \njob-template\n:\n\n    \nname\n:\n \n{name}-{configComponentId}-ci\n\n    \n:\n \n*config_job_defaults\n\n    \nbuilders\n:\n\n        \n-\n \ntest\n\n\n\n-\n \njob-template\n:\n\n    \nname\n:\n \n{name}-{configComponentId}-update\n\n    \n:\n \n*config_job_defaults\n\n    \nbuilders\n:\n\n        \n-\n \nupdate\n\n\n\n\n\n\nGlobal defaults\n\n\nGlobal defaults are defaults that should be global for the jobs you configure for a certain environment. It is the job counterpart of the basic configuration, usually containing variables for the specific environment. For example, url\u2019s, credential id\u2019s, JDK\u2019s etc.\n\n\nExample:\n\nglobal-defaults-localhost.yaml\n\n\n- defaults\n:\n\n    \nname\n:\n \nglobal\n\n    \nflusso-gitlab-url\n:\n \nhttps://gitlab.flusso.nl\n\n    \nnexus-npm-url\n:\n \nhttp://localhost:8081/nexus/content/repositories/npm-internal\n\n    \ndefault-jdk\n:\n \nJDK 1.8\n\n    \njenkinsJobsDefinitionJobName\n:\n \nRnD-Config-JenkinsJobDefinitions-ci\n\n    \ncredentialsId\n:\n \n4f0dfb96-a7b1-421c-a4ea-b6a154f91b08\n\n\n\n\n\n\nJob configuration defaults\n\n\nJob configuration defaults are nothing specific on their own. It refers to using a build in structure from YAML to create basic building blocks to be used by other configuration parts, usually the Templates.\n\n\nExample (definition):\n\n-\n \nconfig_job_defaults\n:\n \nconfig_job_defaults\n\n    \nname\n:\n \nconfig_job_defaults\n\n    \nproject-type\n:\n \nfreestyle\n\n    \ndisabled\n:\n \nfalse\n\n    \nlogrotate\n:\n\n        \ndaysToKeep\n:\n \n7\n\n        \nnumToKeep\n:\n \n5\n\n        \nartifactDaysToKeep\n:\n \n-1\n\n        \nartifactNumToKeep\n:\n \n-1\n\n    \njdk\n:\n \n{default-jdk}\n\n\n\n\nExample (usage):\n\n-\n \njob-template\n:\n\n    \nname\n:\n \n{name}-{configComponentId}-ci\n\n    \n:\n \n*config_job_defaults\n\n\n\n\n\n\nTemplates\n\n\nTemplates are used to define job templates. You define the entirety of the job using global defaults, configuration defaults and where useful refer to placeholders to be filled in by the other downstream configuration items.\n\n\nYou can configure almost every plugin that is available for Jenkins, these are divided in subdivisions which reflect the Jenkins\u2019 job definition sections.\n\n\nFor these subdivision and the available plugins see: \nhttp://docs.openstack.org/infra/jenkins-job-builder/definition.html#modules\n\n\nFor those plugins that are not supported, you can include the raw XML generated by the plugin. For how to do this, see: \nhttp://docs.openstack.org/infra/jenkins-job-builder/definition.html#raw-config\n\n\nExample:\n\n-\n \njob-template\n:\n\n    \nname\n:\n \n{name}-{configComponentId}-ci\n\n    \ndisplay-name\n:\n \n{name}-{configComponentId}-ci\n\n    \ndescription\n:\n \nCI\n \nJob\n \nof\n \n{configComponentId}\n\n    \n:\n \n*config_job_defaults\n\n    \nbuilders\n:\n\n        \n-\n \nshell\n:\n \njenkins-jobs\n \ntest\n \n-r\n \nglobal/:definitions/\n \n-o\n \ncompiled/\n\n    \npublishers\n:\n\n        \n-\n \narchive\n:\n\n            \nartifacts\n:\n \n{filesToArchive_1}\n\n            \nfingerprint\n:\n \ntrue\n\n        \n-\n \narchive\n:\n\n            \nartifacts\n:\n \n{filesToArchive_2}\n\n            \nfingerprint\n:\n \ntrue\n\n        \n-\n \nemail\n:\n\n            \nnotify-every-unstable-build\n:\n \ntrue\n\n            \nsend-to-individuals\n:\n \ntrue\n\n\n\n\n\n\nGroups\n\n\nGroups are used to group together related components that require the same set of jobs. Where you can also specify a similar set of properties, for example, a different JDK to be used.\n\n\nThe name property is mandatory and will be used to match Job definitions.\nThe jobs property is also mandatory and will be used to match Templates for which a Job will be generated per matching Job definition.\n\n\nExample\n\n-\n \njob-group\n:\n\n    \nname\n:\n \n{name}-gulp\n\n    \ngitlab-user\n:\n \njvandergriendt\n\n    \nartifactId\n:\n \n{gulpComponentId}\n\n    \njobs\n:\n\n        \n-\n \n{name}-{gulpComponentId}-ci\n:\n\n        \n-\n \n{name}-{gulpComponentId}-version\n:\n\n        \n-\n \n{name}-{gulpComponentId}-sonar\n:\n\n        \n-\n \n{name}-{gulpComponentId}-publish\n:\n\n        \n-\n \n{name}-{gulpComponentId}-deploy-prep\n:\n\n        \n-\n \n{name}-{gulpComponentId}-deploy\n:\n\n        \n-\n \n{name}-{gulpComponentId}-acceptance\n:\n\n\n\n\n\n\nProjects\n\n\nProjects are used to list the actual Job definitions, which via grouping and Templates get generated, and can obviously be used to define jobs for a specific project.\n\n\nThe name property is mandatory and will be passed along with a Job definition and is generally used to tie job definitions to Groups.\n\n-\n \nproject\n:\n\n    \nname\n:\n \nRnD-Maven\n\n    \njobs\n:\n\n        \n-\n \n{name}-keep\n:\n\n            \ngulpComponentId\n:\n \nkeep-backend\n\n            \ndisplayName\n:\n \nKeep-Backend\n\n\n\n\n\n\nJob definitions\n\n\nJob definitions are what is all about. Although they are part of the Project configuration item I treat them separately.\n\n\nYou list the jobs under a Project and start with the name of the Group it belongs to.\nAfter that, you should define at least a name component to be able to differentiate the different jobs you want. As can be seen in the above examples with the gulpComponentId.\n\n\nExternal configuration files\nSometimes you run into the situation you want to use a multi-line configuration for a plugin, or a set of commands. Or, used at in different configurations or templates.\n\n\nThen you run into the situation that it is very difficult to manage in them neatly inside YAML configuration files. For this situation you are able to simply include a text file, via a native YAML construct. See: \nhttp://docs.openstack.org/infra/jenkins-job-builder/definition.html#module-jenkins_jobs.local_yaml\n\n\nFor example\n\n-\n \njob\n:\n\n    \nname\n:\n \ntest-job-include-raw-1\n\n    \nbuilders\n:\n\n      \n-\n \nshell\n:\n\n          \n!include-raw\n \ninclude-raw001-hello-world.sh\n\n      \n-\n \nshell\n:\n\n          \n!include-raw\n \ninclude-raw001-vars.sh\n\n\n\n\n\f\n\n\nUsage\n\n\nThe information to how you use the tool is very well explained in the documentation. See \nhttp://docs.openstack.org/infra/jenkins-job-builder/installation.html#running\n\nAutomated maintenance\nIf all the jobs you can administer are done via Jenkins Job Builder, you can start to automate the maintenance of these jobs.\n\n\nSimply make jobs that poll/push on the code base where you have your Jenkins Job Builder configuration files.\n\n\nExample\n\n-\n \nconfig_job_defaults\n:\n \nconfig_job_defaults\n\n    \nname\n:\n \nconfig_job_defaults\n\n    \nproject-type\n:\n \nfreestyle\n\n    \ndisabled\n:\n \nfalse\n\n    \nlogrotate\n:\n\n        \ndaysToKeep\n:\n \n7\n\n        \nnumToKeep\n:\n \n5\n\n        \nartifactDaysToKeep\n:\n \n-1\n\n        \nartifactNumToKeep\n:\n \n-1\n\n    \njdk\n:\n \n{default-jdk}\n\n    \ntriggers\n:\n\n        \n-\n \npollscm\n:\n \nH/15\n \n*\n \n*\n \n*\n \n*\n\n    \nscm\n:\n\n        \n-\n \ngit\n:\n\n            \nurl\n:\n \n{flusso-gitlab-url}/{gitlab-user}/{componentGitName}.git\n\n            \ncredentials-id\n:\n \n{credentialsId}\n\n    \npublishers\n:\n\n        \n-\n \nemail\n:\n\n            \nnotify-every-unstable-build\n:\n \ntrue\n\n            \nsend-to-individuals\n:\n \ntrue\n\n\n\n-\n \njob-template\n:\n\n    \nname\n:\n \n{name}-{configComponentId}-ci\n\n    \ndisplay-name\n:\n \n{name}-{configComponentId}-ci\n\n    \ndescription\n:\n \nCI\n \nJob\n \nof\n \n{configComponentId}\n\n    \n:\n \n*config_job_defaults\n\n    \nbuilders\n:\n\n        \n-\n \nshell\n:\n \njenkins-jobs\n \ntest\n \n-r\n \nglobal/:definitions/\n \n-o\n \ncompiled/\n\n    \npublishers\n:\n\n        \n-\n \narchive\n:\n\n            \nartifacts\n:\n \n{filesToArchive_1}\n\n            \nfingerprint\n:\n \ntrue\n\n        \n-\n \narchive\n:\n\n            \nartifacts\n:\n \n{filesToArchive_2}\n\n            \nfingerprint\n:\n \ntrue\n\n        \n-\n \nemail\n:\n\n            \nnotify-every-unstable-build\n:\n \ntrue\n\n            \nsend-to-individuals\n:\n \ntrue\n\n\n\n-\n \njob-template\n:\n\n    \nname\n:\n \n{name}-{configComponentId}-x\n\n    \ndisplay-name\n:\n \n{name}-{configComponentId}-execute\n\n    \ndescription\n:\n \nExecutor\n \nJob\n \nof\n \n{configComponentId},\n \nit\n \nwill\n \nexecute\n \nthe\n \nupdate\n \nand\n \ndelete\n \nold\n \ncommand\n\n    \n:\n \n*config_job_defaults\n\n    \nbuilders\n:\n\n        \n-\n \nshell\n:\n \njenkins-jobs\n \n--conf\n \nconfiguration/localhost.ini\n \nupdate\n \n--delete-old\n \n-r\n \nglobal/:definitions/\n\n\n\n-\n \njob-group\n:\n\n    \nname\n:\n \n{name}-config\n\n    \ngitlab-user\n:\n \njvandergriendt\n\n    \njobs\n:\n\n        \n-\n \n{name}-{configComponentId}-ci\n:\n\n        \n-\n \n{name}-{configComponentId}-x\n:\n\n\n\n-\n \nproject\n:\n\n    \nname\n:\n \nRnD-Config\n\n    \njobs\n:\n\n        \n-\n \n{name}-config\n:\n\n            \nconfigComponentId\n:\n \nJenkinsJobDefinitions\n\n            \ncomponentGitName\n:\n \njenkins-job-definitions\n\n            \nfilesToArchive_1\n:\n \nscripts/*.sh\n\n            \nfilesToArchive_2\n:\n \nmaven/settings.xml\n\n\n\n\n\n\nTips \n Trick\n\n\nAs the documentation is so extensive, it can sometimes be difficult to figure out what would be a good way to deal with some constructs.\nComponent identifier property\nOne important thing to keep in mind is that in order to create a whole set of jobs via the groups and templates it imperative to have a component* identifier property.\n\n\nThis way you can define hundreds of jobs in a project, dozens of groups and dozens of templates and generate thousands of unique individual jobs. Scale does not actually matter in this case, if you have more than one job in a project you will need this property. If the jobs that will be generated will not differ the execution will fail.\n\n\n\n\nBulk\n\n\nyou can combine multiple files or even entire folder structures together in a single call. For example, if you manage all the jobs of a company or a department and configure them in separate files.\n\n\n\n\nFor example\n\njenkins-jobs --conf configuration/localhost.ini update --delete-old -r global/:definitions/", 
            "title": "JenkinsJobsBuilder"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#jenkins-job-builder", 
            "text": "The configuration setup of Jenkins Job Builder is composed of two main categories. Basic configuration and job configuration. Job configuration can be further split into several sub categories.", 
            "title": "Jenkins Job Builder"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#basic-configuration", 
            "text": "In the basic configuration you will have to specify how the Jenkins Job Builder CLI can connect to the Jenkins instance you want to configure and how it should act.  To use such a configuration file, you add --conf   to the CLI command.  Example: localhost.ini\n[job_builder]\nignore_cache=True\nkeep_descriptions=False\ninclude_path=.:scripts:~/git/\nrecursive=False\nexclude=.*:manual:./development\nallow_duplicates=False\n\n[jenkins]\n#user=jenkins\n#password=\nurl=http://localhost:8080/ \n\nFor more information see  http://docs.openstack.org/infra/jenkins-job-builder/installation.html .", 
            "title": "Basic Configuration"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#job-configuration", 
            "text": "The configuration for configuring the jobs consists of several distinct parts which can all be in the same file or can be distributed in their own respected files.  These different parts can also be split into two different categories, those that are strictly linked within the configuration - via template matching - and those that are separate.  Separate:\n* Macro\u2019s\n* Global defaults\n* Job configuration defaults\n* External configuration files  Linked:\n* Templates\n* Groups\n* Projects\n* Job definitions   Here\u2019s a schematic representation on how they are linked.\nExampe in YAML config: -   job-template : \n     name :   {name}-{configComponentId}-ci \n     description :   CI   Job   of   {configComponentId} \n     :   *config_job_defaults \n     builders : \n         -   shell :   jenkins-jobs   test   -r   global/:definitions/   -o   compiled/  -   job-template : \n     name :   {name}-{configComponentId}-execute \n     description :   Executor   Job   of   {configComponentId} \n     :   *config_job_defaults \n     builders : \n         -   shell :   jenkins-jobs   --conf   configuration/localhost.ini   update   definitions/  -   job-group : \n     name :   {name}-config \n     gitlab-user :   jvandergriendt \n     jobs : \n         -   {name}-{configComponentId}-ci : \n         -   {name}-{configComponentId}-execute :  -   project : \n     name :   RnD-Config \n     jobs : \n         -   {name}-config : \n             configComponentId :   JenkinsJobDefinitions  \n\nThe above will result in the following jobs:\nRnD-Config-JenkinsJobDefinitions-ci\nRnD-Config-JenkinsJobDefinitions-execute", 
            "title": "Job Configuration"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#macros", 
            "text": "Macro\u2019s are what the name implies, a group of related commands which can be invoked by the group. In Jenkins Job Builder this means you can define specific configurations for a component type (e.g. builders, paramters, publishes etc).  A component has a name and a macro name. In general the component name is plural and the macro name is singular. As can be seen in the examples below.  Here\u2019s an example: # The  add  macro takes a  number  parameter and will creates a  # job which prints  Adding   followed by the  number  parameter:  -   builder : \n     name :   add \n     builders : \n      -   shell :   echo   Adding   {number}  # A specialized macro  addtwo  reusing the  add  macro but with  # a  number  parameter hardcoded to  two :  -   builder : \n     name :   addtwo \n     builders : \n      -   add : \n         number :   two  # Glue to have Jenkins Job Builder to expand this YAML example:  -   job : \n     name :   testingjob \n     builders : \n      # The specialized macro: \n      -   addtwo \n      # Generic macro call with a parameter \n      -   add : \n         number :   ZERO \n      # Generic macro called without a parameter. Never do this! \n      # See below for the resulting wrong output :( \n      -   add    To expand the schematic representation, you will get the following.  -   builder : \n     name :   test \n     builders : \n      -   shell :   jenkins-jobs   test   -r   global/:definitions/   -o   compiled/  -   builder : \n     name :   update \n     builders : \n      -   shell :   jenkins-jobs   --conf   config.ini   update   -r   global/:definitions/  -   job-template : \n     name :   {name}-{configComponentId}-ci \n     :   *config_job_defaults \n     builders : \n         -   test  -   job-template : \n     name :   {name}-{configComponentId}-update \n     :   *config_job_defaults \n     builders : \n         -   update", 
            "title": "Macro\u2019s"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#global-defaults", 
            "text": "Global defaults are defaults that should be global for the jobs you configure for a certain environment. It is the job counterpart of the basic configuration, usually containing variables for the specific environment. For example, url\u2019s, credential id\u2019s, JDK\u2019s etc.  Example: global-defaults-localhost.yaml  - defaults : \n     name :   global \n     flusso-gitlab-url :   https://gitlab.flusso.nl \n     nexus-npm-url :   http://localhost:8081/nexus/content/repositories/npm-internal \n     default-jdk :   JDK 1.8 \n     jenkinsJobsDefinitionJobName :   RnD-Config-JenkinsJobDefinitions-ci \n     credentialsId :   4f0dfb96-a7b1-421c-a4ea-b6a154f91b08", 
            "title": "Global defaults"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#job-configuration-defaults", 
            "text": "Job configuration defaults are nothing specific on their own. It refers to using a build in structure from YAML to create basic building blocks to be used by other configuration parts, usually the Templates.  Example (definition): -   config_job_defaults :   config_job_defaults \n     name :   config_job_defaults \n     project-type :   freestyle \n     disabled :   false \n     logrotate : \n         daysToKeep :   7 \n         numToKeep :   5 \n         artifactDaysToKeep :   -1 \n         artifactNumToKeep :   -1 \n     jdk :   {default-jdk}  \n\nExample (usage): -   job-template : \n     name :   {name}-{configComponentId}-ci \n     :   *config_job_defaults", 
            "title": "Job configuration defaults"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#templates", 
            "text": "Templates are used to define job templates. You define the entirety of the job using global defaults, configuration defaults and where useful refer to placeholders to be filled in by the other downstream configuration items.  You can configure almost every plugin that is available for Jenkins, these are divided in subdivisions which reflect the Jenkins\u2019 job definition sections.  For these subdivision and the available plugins see:  http://docs.openstack.org/infra/jenkins-job-builder/definition.html#modules  For those plugins that are not supported, you can include the raw XML generated by the plugin. For how to do this, see:  http://docs.openstack.org/infra/jenkins-job-builder/definition.html#raw-config  Example: -   job-template : \n     name :   {name}-{configComponentId}-ci \n     display-name :   {name}-{configComponentId}-ci \n     description :   CI   Job   of   {configComponentId} \n     :   *config_job_defaults \n     builders : \n         -   shell :   jenkins-jobs   test   -r   global/:definitions/   -o   compiled/ \n     publishers : \n         -   archive : \n             artifacts :   {filesToArchive_1} \n             fingerprint :   true \n         -   archive : \n             artifacts :   {filesToArchive_2} \n             fingerprint :   true \n         -   email : \n             notify-every-unstable-build :   true \n             send-to-individuals :   true", 
            "title": "Templates"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#groups", 
            "text": "Groups are used to group together related components that require the same set of jobs. Where you can also specify a similar set of properties, for example, a different JDK to be used.  The name property is mandatory and will be used to match Job definitions.\nThe jobs property is also mandatory and will be used to match Templates for which a Job will be generated per matching Job definition.  Example -   job-group : \n     name :   {name}-gulp \n     gitlab-user :   jvandergriendt \n     artifactId :   {gulpComponentId} \n     jobs : \n         -   {name}-{gulpComponentId}-ci : \n         -   {name}-{gulpComponentId}-version : \n         -   {name}-{gulpComponentId}-sonar : \n         -   {name}-{gulpComponentId}-publish : \n         -   {name}-{gulpComponentId}-deploy-prep : \n         -   {name}-{gulpComponentId}-deploy : \n         -   {name}-{gulpComponentId}-acceptance :", 
            "title": "Groups"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#projects", 
            "text": "Projects are used to list the actual Job definitions, which via grouping and Templates get generated, and can obviously be used to define jobs for a specific project.  The name property is mandatory and will be passed along with a Job definition and is generally used to tie job definitions to Groups. -   project : \n     name :   RnD-Maven \n     jobs : \n         -   {name}-keep : \n             gulpComponentId :   keep-backend \n             displayName :   Keep-Backend", 
            "title": "Projects"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#job-definitions", 
            "text": "Job definitions are what is all about. Although they are part of the Project configuration item I treat them separately.  You list the jobs under a Project and start with the name of the Group it belongs to.\nAfter that, you should define at least a name component to be able to differentiate the different jobs you want. As can be seen in the above examples with the gulpComponentId.  External configuration files\nSometimes you run into the situation you want to use a multi-line configuration for a plugin, or a set of commands. Or, used at in different configurations or templates.  Then you run into the situation that it is very difficult to manage in them neatly inside YAML configuration files. For this situation you are able to simply include a text file, via a native YAML construct. See:  http://docs.openstack.org/infra/jenkins-job-builder/definition.html#module-jenkins_jobs.local_yaml  For example -   job : \n     name :   test-job-include-raw-1 \n     builders : \n       -   shell : \n           !include-raw   include-raw001-hello-world.sh \n       -   shell : \n           !include-raw   include-raw001-vars.sh", 
            "title": "Job definitions"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#usage", 
            "text": "The information to how you use the tool is very well explained in the documentation. See  http://docs.openstack.org/infra/jenkins-job-builder/installation.html#running \nAutomated maintenance\nIf all the jobs you can administer are done via Jenkins Job Builder, you can start to automate the maintenance of these jobs.  Simply make jobs that poll/push on the code base where you have your Jenkins Job Builder configuration files.  Example -   config_job_defaults :   config_job_defaults \n     name :   config_job_defaults \n     project-type :   freestyle \n     disabled :   false \n     logrotate : \n         daysToKeep :   7 \n         numToKeep :   5 \n         artifactDaysToKeep :   -1 \n         artifactNumToKeep :   -1 \n     jdk :   {default-jdk} \n     triggers : \n         -   pollscm :   H/15   *   *   *   * \n     scm : \n         -   git : \n             url :   {flusso-gitlab-url}/{gitlab-user}/{componentGitName}.git \n             credentials-id :   {credentialsId} \n     publishers : \n         -   email : \n             notify-every-unstable-build :   true \n             send-to-individuals :   true  -   job-template : \n     name :   {name}-{configComponentId}-ci \n     display-name :   {name}-{configComponentId}-ci \n     description :   CI   Job   of   {configComponentId} \n     :   *config_job_defaults \n     builders : \n         -   shell :   jenkins-jobs   test   -r   global/:definitions/   -o   compiled/ \n     publishers : \n         -   archive : \n             artifacts :   {filesToArchive_1} \n             fingerprint :   true \n         -   archive : \n             artifacts :   {filesToArchive_2} \n             fingerprint :   true \n         -   email : \n             notify-every-unstable-build :   true \n             send-to-individuals :   true  -   job-template : \n     name :   {name}-{configComponentId}-x \n     display-name :   {name}-{configComponentId}-execute \n     description :   Executor   Job   of   {configComponentId},   it   will   execute   the   update   and   delete   old   command \n     :   *config_job_defaults \n     builders : \n         -   shell :   jenkins-jobs   --conf   configuration/localhost.ini   update   --delete-old   -r   global/:definitions/  -   job-group : \n     name :   {name}-config \n     gitlab-user :   jvandergriendt \n     jobs : \n         -   {name}-{configComponentId}-ci : \n         -   {name}-{configComponentId}-x :  -   project : \n     name :   RnD-Config \n     jobs : \n         -   {name}-config : \n             configComponentId :   JenkinsJobDefinitions \n             componentGitName :   jenkins-job-definitions \n             filesToArchive_1 :   scripts/*.sh \n             filesToArchive_2 :   maven/settings.xml", 
            "title": "Usage"
        }, 
        {
            "location": "/jenkins-jobs/jenkins-jobs-builder/#tips-trick", 
            "text": "As the documentation is so extensive, it can sometimes be difficult to figure out what would be a good way to deal with some constructs.\nComponent identifier property\nOne important thing to keep in mind is that in order to create a whole set of jobs via the groups and templates it imperative to have a component* identifier property.  This way you can define hundreds of jobs in a project, dozens of groups and dozens of templates and generate thousands of unique individual jobs. Scale does not actually matter in this case, if you have more than one job in a project you will need this property. If the jobs that will be generated will not differ the execution will fail.   Bulk  you can combine multiple files or even entire folder structures together in a single call. For example, if you manage all the jobs of a company or a department and configure them in separate files.   For example jenkins-jobs --conf configuration/localhost.ini update --delete-old -r global/:definitions/", 
            "title": "Tips &amp; Trick"
        }, 
        {
            "location": "/jenkins-pipeline/declarative-pipeline/", 
            "text": "Declarative Pipeline\n\n\nDeclarative Pipeline is a relatively recent addition to Jenkins Pipeline [1] which presents a more simplified and opinionated syntax on top of the Pipeline sub-systems.\n\n\nAll valid Declarative Pipelines must be enclosed within a pipeline block, for example:\n\n\npipeline\n \n{\n\n    \n/* insert Declarative Pipeline here */\n\n\n}\n\n\n\n\n\n\nHello World Example\n\n\npipeline\n \n{\n\n    \nagent\n \n{\n \ndocker\n \npython:3.5.1\n \n}\n\n    \nstages\n \n{\n\n        \nstage\n(\nbuild\n)\n \n{\n\n            \nsteps\n \n{\n\n                \nsh\n \npython --version\n\n            \n}\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\nMKDocs Build Example\n\n\npipeline\n \n{\n\n    \nagent\n \nnone\n\n    \noptions\n \n{\n\n        \ntimeout\n(\ntime:\n \n10\n,\n \nunit:\n \nMINUTES\n)\n\n        \ntimestamps\n()\n\n        \nbuildDiscarder\n(\nlogRotator\n(\nnumToKeepStr:\n \n5\n))\n\n    \n}\n\n    \nstages\n \n{\n\n        \nstage\n(\nPrepare\n){\n\n            \nagent\n \n{\n \nlabel\n \ndocker\n \n}\n\n            \nsteps\n \n{\n\n                \nparallel\n \n(\n\n                    \nClean:\n \n{\n\n                        \ndeleteDir\n()\n\n                    \n},\n\n                    \nNotifySlack:\n \n{\n\n                        \nslackSend\n \nchannel:\n \ncicd\n,\n \ncolor:\n \n#FFFF00\n,\n \nmessage:\n \nSTARTED: Job \n${env.JOB_NAME} [${env.BUILD_NUMBER}]\n (${env.BUILD_URL})\n\n                    \n}\n\n                \n)\n\n            \n}\n\n        \n}\n\n        \nstage\n(\nCheckout\n){\n\n            \nagent\n \n{\n \nlabel\n \ndocker\n \n}\n\n            \nsteps\n \n{\n\n                \ngit\n \ncredentialsId:\n \n355df378-e726-4abd-90fa-e723c5c21ad5\n,\n \nurl:\n \ngit@gitlab.flusso.nl:CICD/ci-cd-docs.git\n\n                \nscript\n \n{\n\n                    \nenv\n.\nGIT_COMMIT_HASH\n \n=\n \nsh\n \nreturnStdout:\n \ntrue\n,\n \nscript:\n \ngit rev-parse --verify HEAD\n\n                \n}\n\n            \n}\n\n        \n}\n\n        \nstage\n(\nBuild Docs\n)\n \n{\n\n            \nagent\n \n{\n\n                \ndocker\n \n{\n\n                    \nimage\n \ncaladreas/mkdocs-docker-build-container\n\n                    \nlabel\n \ndocker\n\n                \n}\n\n            \n}\n\n            \nsteps\n \n{\n\n                \nsh\n \nmkdocs build\n\n            \n}\n\n        \n}\n\n        \nstage\n(\nPrepare Docker Image\n){\n\n            \nagent\n \n{\n \nlabel\n \ndocker\n \n}\n\n            \nsteps\n \n{\n\n                \nparallel\n \n(\n\n                    \nTestDockerfile:\n \n{\n\n                        \nscript\n \n{\n\n                            \ndef\n \nlintResult\n \n=\n \nsh\n \nreturnStdout:\n \ntrue\n,\n \nscript:\n \ndocker run --rm -i lukasmartinelli/hadolint \n Dockerfile\n\n                            \nif\n \n(\nlintResult\n.\ntrim\n()\n \n==\n \n)\n \n{\n\n                                \nprintln\n \nLint finished with no errors\n\n                            \n}\n \nelse\n \n{\n\n                                \nprintln\n \nError found in Lint\n\n                                \nprintln\n \n${lintResult}\n\n                                \ncurrentBuild\n.\nresult\n \n=\n \nUNSTABLE\n\n                            \n}\n\n                        \n}\n\n                    \n},\n \n// end test dockerfile\n\n                    \nBuildImage:\n \n{\n\n                        \nsh\n \nchmod +x build.sh\n\n                        \nsh\n \n./build.sh\n\n                    \n}\n \n                \n)\n\n            \n}\n\n            \npost\n \n{\n\n                \nsuccess\n \n{\n\n                    \nsh\n \nchmod +x push.sh\n\n                    \nsh\n \n./push.sh\n\n                \n}\n\n            \n}\n\n        \n}\n\n        \nstage\n(\nUpdate Docker Container\n)\n \n{\n\n            \nagent\n \n{\n \nlabel\n \ndocker\n \n}\n\n            \nsteps\n \n{\n\n                \nsh\n \nchmod +x container-update.sh\n\n                \nsh\n \n./container-update.sh ${env.BUILD_URL} ${env.GIT_COMMIT_HASH}\n\n            \n}\n\n        \n}\n\n    \n}\n\n    \npost\n \n{\n\n        \nsuccess\n \n{\n\n            \nslackSend\n \nchannel:\n \ncicd\n,\n \ncolor:\n \n#00FF00\n,\n \nmessage:\n \nSUCCESSFUL: Job \n${env.JOB_NAME} [${env.BUILD_NUMBER}]\n (${env.BUILD_URL})\n\n        \n}\n\n        \nfailure\n \n{\n\n            \nslackSend\n \nchannel:\n \ncicd\n,\n \ncolor:\n \n#FF0000\n,\n \nmessage:\n \nFAILED: Job \n${env.JOB_NAME} [${env.BUILD_NUMBER}]\n (${env.BUILD_URL})\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\nResources\n\n\n\n\nSyntax Reference\n\n\nGetting started\n\n\nNotifications", 
            "title": "Pipeline (declarative)"
        }, 
        {
            "location": "/jenkins-pipeline/declarative-pipeline/#declarative-pipeline", 
            "text": "Declarative Pipeline is a relatively recent addition to Jenkins Pipeline [1] which presents a more simplified and opinionated syntax on top of the Pipeline sub-systems.  All valid Declarative Pipelines must be enclosed within a pipeline block, for example:  pipeline   { \n     /* insert Declarative Pipeline here */  }", 
            "title": "Declarative Pipeline"
        }, 
        {
            "location": "/jenkins-pipeline/declarative-pipeline/#hello-world-example", 
            "text": "pipeline   { \n     agent   {   docker   python:3.5.1   } \n     stages   { \n         stage ( build )   { \n             steps   { \n                 sh   python --version \n             } \n         } \n     }  }", 
            "title": "Hello World Example"
        }, 
        {
            "location": "/jenkins-pipeline/declarative-pipeline/#mkdocs-build-example", 
            "text": "pipeline   { \n     agent   none \n     options   { \n         timeout ( time:   10 ,   unit:   MINUTES ) \n         timestamps () \n         buildDiscarder ( logRotator ( numToKeepStr:   5 )) \n     } \n     stages   { \n         stage ( Prepare ){ \n             agent   {   label   docker   } \n             steps   { \n                 parallel   ( \n                     Clean:   { \n                         deleteDir () \n                     }, \n                     NotifySlack:   { \n                         slackSend   channel:   cicd ,   color:   #FFFF00 ,   message:   STARTED: Job  ${env.JOB_NAME} [${env.BUILD_NUMBER}]  (${env.BUILD_URL}) \n                     } \n                 ) \n             } \n         } \n         stage ( Checkout ){ \n             agent   {   label   docker   } \n             steps   { \n                 git   credentialsId:   355df378-e726-4abd-90fa-e723c5c21ad5 ,   url:   git@gitlab.flusso.nl:CICD/ci-cd-docs.git \n                 script   { \n                     env . GIT_COMMIT_HASH   =   sh   returnStdout:   true ,   script:   git rev-parse --verify HEAD \n                 } \n             } \n         } \n         stage ( Build Docs )   { \n             agent   { \n                 docker   { \n                     image   caladreas/mkdocs-docker-build-container \n                     label   docker \n                 } \n             } \n             steps   { \n                 sh   mkdocs build \n             } \n         } \n         stage ( Prepare Docker Image ){ \n             agent   {   label   docker   } \n             steps   { \n                 parallel   ( \n                     TestDockerfile:   { \n                         script   { \n                             def   lintResult   =   sh   returnStdout:   true ,   script:   docker run --rm -i lukasmartinelli/hadolint   Dockerfile \n                             if   ( lintResult . trim ()   ==   )   { \n                                 println   Lint finished with no errors \n                             }   else   { \n                                 println   Error found in Lint \n                                 println   ${lintResult} \n                                 currentBuild . result   =   UNSTABLE \n                             } \n                         } \n                     },   // end test dockerfile \n                     BuildImage:   { \n                         sh   chmod +x build.sh \n                         sh   ./build.sh \n                     }  \n                 ) \n             } \n             post   { \n                 success   { \n                     sh   chmod +x push.sh \n                     sh   ./push.sh \n                 } \n             } \n         } \n         stage ( Update Docker Container )   { \n             agent   {   label   docker   } \n             steps   { \n                 sh   chmod +x container-update.sh \n                 sh   ./container-update.sh ${env.BUILD_URL} ${env.GIT_COMMIT_HASH} \n             } \n         } \n     } \n     post   { \n         success   { \n             slackSend   channel:   cicd ,   color:   #00FF00 ,   message:   SUCCESSFUL: Job  ${env.JOB_NAME} [${env.BUILD_NUMBER}]  (${env.BUILD_URL}) \n         } \n         failure   { \n             slackSend   channel:   cicd ,   color:   #FF0000 ,   message:   FAILED: Job  ${env.JOB_NAME} [${env.BUILD_NUMBER}]  (${env.BUILD_URL}) \n         } \n     }  }", 
            "title": "MKDocs Build Example"
        }, 
        {
            "location": "/jenkins-pipeline/declarative-pipeline/#resources", 
            "text": "Syntax Reference  Getting started  Notifications", 
            "title": "Resources"
        }, 
        {
            "location": "/jenkins-pipeline/groovy-pipeline/", 
            "text": "Jenkins Pipelines\n\n\n\n\nWarning\n\n\nThis style of pipeline definition is deprecated.\nWhen possible, please use the declarative version.\n\n\n\n\n\n\nJenkins Pipeline is a suite of plugins which supports implementing and integrating continuous delivery pipelines into Jenkins. Pipeline provides an extensible set of tools for modeling simple-to-complex delivery pipelines \"as code\" via the Pipeline DSL.\n\n\n\n\nThere are two ways to create pipelines in Jenkins.\nEither via the \nGroovy DSL\n or via the \nDeclarative pipeline\n.\n\n\nFor more information about the declarative pipeline, read the \nnext page\n.\n\n\nHello World Example\n\n\nnode\n \n{\n\n    \ntimestamps\n \n{\n\n        \nstage\n \n(\nMy FIrst Stage\n)\n \n{\n\n            \nif\n \n(\nisUnix\n())\n \n{\n\n                \nsh\n \necho \nthis is Unix!\n\n            \n}\n \nelse\n \n{\n\n                \nbat\n \necho \nthis is windows\n\n            \n}\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\nResources\n\n\n\n\nGetting started\n\n\nBest practices\n\n\nBest practices for scaling\n\n\nPossible Steps", 
            "title": "Groovy DSL Pipeline"
        }, 
        {
            "location": "/jenkins-pipeline/groovy-pipeline/#jenkins-pipelines", 
            "text": "Warning  This style of pipeline definition is deprecated.\nWhen possible, please use the declarative version.    Jenkins Pipeline is a suite of plugins which supports implementing and integrating continuous delivery pipelines into Jenkins. Pipeline provides an extensible set of tools for modeling simple-to-complex delivery pipelines \"as code\" via the Pipeline DSL.   There are two ways to create pipelines in Jenkins.\nEither via the  Groovy DSL  or via the  Declarative pipeline .  For more information about the declarative pipeline, read the  next page .", 
            "title": "Jenkins Pipelines"
        }, 
        {
            "location": "/jenkins-pipeline/groovy-pipeline/#hello-world-example", 
            "text": "node   { \n     timestamps   { \n         stage   ( My FIrst Stage )   { \n             if   ( isUnix ())   { \n                 sh   echo  this is Unix! \n             }   else   { \n                 bat   echo  this is windows \n             } \n         } \n     }  }", 
            "title": "Hello World Example"
        }, 
        {
            "location": "/jenkins-pipeline/groovy-pipeline/#resources", 
            "text": "Getting started  Best practices  Best practices for scaling  Possible Steps", 
            "title": "Resources"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/", 
            "text": "Global Shared Library\n\n\nhttps://jenkins.io/doc/book/pipeline/shared-libraries/\n\n\nWhen you're making pipelines on Jenkins you will run into the situation that you will want to stay \nDRY\n.\nTo share pipeline code there are several ways.\n\n\n\n\nSCM:\n Have a pipeline dsl script in a SCM and load it from there\n\n\nPlugin:\n A Jenkins plugin that you can call via the pipeline dsl\n\n\nGlobal Workflow Library:\n There is a global library for pipeline dsl scripts in the Jekins master \nPreferred solution\n\n\n\n\nPlease read the \ndocumentation\n to get a basic idea.\n\n\n\n\nDanger\n\n\nWhen using a Global Library you will always have to import something from this library.\nThis doesn't make sense when you online use functions (via the vars folder).\nIn this case, you have to import nothing, which you do via: \"_\"\n\n\n\n\n@Library\n(\nFlussoGlobal\n)\n\n\nimport\n \nnl.flusso.Utilities\n\n\n\n\n\n\n@Library\n(\nFlussoGlobal\n)\n \n_\n\n\n\n\n\n\nLibrary Directory structure\n\n\nThe directory structure of a shared library repository is as follows:\n\n\n(root)\n +- src                     # Groovy source files\n |   +- org\n |       +- foo\n |           +- Bar.groovy  # for org.foo.Bar class\n +- vars\n |   +- foo.groovy          # for global \nfoo\n variable/function\n |   +- foo.txt             # help for \nfoo\n variable/function\n +- resources               # resource files (external libraries only)\n |   +- org\n |       +- foo\n |           +- bar.json    # static helper data for org.foo.Bar\n\n\n\n\n\nThe \nsrc\n directory should look like standard Java source directory structure.\nThis directory is added to the classpath when executing Pipelines.\n\n\nThe \nvars\n directory hosts scripts that define global variables accessible from\nPipeline scripts.\nThe basename of each \n*.groovy\n file should be a Groovy (~ Java) identifier, conventionally \ncamelCased\n.\nThe matching \n*.txt\n, if present, can contain documentation, processed through the system\u2019s configured markup formatter\n(so may really be HTML, Markdown, etc., though the \ntxt\n extension is required).\n\n\nThe Groovy source files in these directories get the same \u201cCPS transformation\u201d as your Pipeline scripts.\n\n\nA \nresources\n directory allows the \nlibraryResource\n step to be used from an external library to load associated non-Groovy files.\nCurrently this feature is not supported for internal libraries.\n\n\nOther directories under the root are reserved for future enhancements.\n\n\nConfigure libraries in Jenkins\n\n\nThe a Jenkins Master you can configure the Global Pipeline Libraries.\n\n\nYou can find this in: Manage Jenkins -\n Configure System -\n Global Pipeline Libraries\n\n\nYou can configure multiple libraries, where the there is a preference for Git repositories.\nYou can select a default version (for example: the master branch), and either allow or disallow overrides to this.\n\n\nTo be able to use a different version, you would use the @\n in case of Git.\n\n\n@Library\n(\nFlussoGlobal\n@my\n-\nfeature\n-\nbranch\n)\n\n\n\n\n\n\n\n\nHelloWorld Example\n\n\n\n\nCreate Git repository (see below for structure)\n\n\nConfigure this Git repository as an \"Global Pipeline Libraries\" entry\n\n\nName: FlussoGlobal\n\n\nDefault Version: master\n\n\nModern SCM: git\n\n\nProject repository: \n:CICD/jenkins-pipeline-library.git\n\n\nCreate the resources you want in the git repository\n\n\nUse the library in a pipeline\n\n\n\n\nUtil Class (class) Example\n\n\n#!/usr/bin/groovy\n\n\n#\n/src/\nnl\n/flusso/\nUtilities\n.\ngroovy\n\n\npackage\n \nnl\n.\nflusso\n\n\n\nimport\n \njava.io.Serializable\n\n\n\nclass\n \nUtilities\n \nimplements\n \nSerializable\n \n{\n\n  \ndef\n \nsteps\n\n\n  \nUtilities\n(\nsteps\n)\n \n{\nthis\n.\nsteps\n \n=\n \nsteps\n}\n\n\n  \ndef\n \nsayHello\n(\nString\n \nname\n)\n \n{\n\n    \nsteps\n.\nsh\n \necho $name\n\n  \n}\n\n\n}\n\n\n\n\n\n\n@Library\n(\nFlussoGlobal\n)\n\n\nimport\n \nnl.flusso.Utilities\n\n\n\ndef\n \nutils\n \n=\n \nnew\n \nUtilities\n(\nsteps\n)\n\n\n\nnode\n \n{\n\n    \nString\n \nname\n \n=\n \nJoost\n\n    \nutils\n.\nsayHello\n(\nname\n)\n\n\n}\n\n\n\n\n\n\nUtil method (var) Example\n\n\n#!/usr/bin/groovy\n\n\n#\n/vars/\nsayHello\n.\ngroovy\n\n\ndef\n \ncall\n(\nname\n)\n \n{\n\n    \n// you can call any valid step functions from your code, just like you can from Pipeline scripts\n\n    \necho\n \nHello world, ${name}\n\n\n}\n\n\n\n\n\n\n@Library\n(\nFlussoGlobal\n)\n \n_\n\n\nnode\n \n{\n\n    \nString\n \nname\n \n=\n \nJoost\n\n    \nsayHello\n \nname\n\n\n}\n\n\n\n\n\n\nCombining libraries\n\n\nLets say you want to want to have a core library and multiple specific libraries that utilize these. There are several to do this, we will show two.\n\n\nImport both\n\n\nOne way is to explicitly import both libraries in the Jenkinsfile.\n\n\n@Library\n([\ngithub.com/joostvdg/jenkins-pipeline-lib\n,\ngithub.com/joostvdg/jenkins-pipeline-go\n])\n \n_\n\n\n\n\n\n\nCon:\n\n\n\n\nyou have to import all the required libraries yourself\n\n\n\n\nPro:\n\n\n\n\nyou can specify the versions of each\n\n\n\n\nImplicit Import + Explicit Import\n\n\nYou can also configure the core (in this case jenkins-pipeline-lib) as \"loaded implicitly\".\nThis will make anything from this library available by default.\n\n\nBe careful with the naming of the vars though!\n\n\nThe resulting Jenkinsfile would then be.\n\n\n@Library\n(\ngithub.com/joostvdg/jenkins-pipeline-go\n)\n \n_\n\n\n\n\n\n\nResources\n\n\n\n\nimplement-reusable-function-call", 
            "title": "Pipeline Libraries"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/#global-shared-library", 
            "text": "https://jenkins.io/doc/book/pipeline/shared-libraries/  When you're making pipelines on Jenkins you will run into the situation that you will want to stay  DRY .\nTo share pipeline code there are several ways.   SCM:  Have a pipeline dsl script in a SCM and load it from there  Plugin:  A Jenkins plugin that you can call via the pipeline dsl  Global Workflow Library:  There is a global library for pipeline dsl scripts in the Jekins master  Preferred solution   Please read the  documentation  to get a basic idea.   Danger  When using a Global Library you will always have to import something from this library.\nThis doesn't make sense when you online use functions (via the vars folder).\nIn this case, you have to import nothing, which you do via: \"_\"   @Library ( FlussoGlobal )  import   nl.flusso.Utilities   @Library ( FlussoGlobal )   _", 
            "title": "Global Shared Library"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/#library-directory-structure", 
            "text": "The directory structure of a shared library repository is as follows:  (root)\n +- src                     # Groovy source files\n |   +- org\n |       +- foo\n |           +- Bar.groovy  # for org.foo.Bar class\n +- vars\n |   +- foo.groovy          # for global  foo  variable/function\n |   +- foo.txt             # help for  foo  variable/function\n +- resources               # resource files (external libraries only)\n |   +- org\n |       +- foo\n |           +- bar.json    # static helper data for org.foo.Bar  The  src  directory should look like standard Java source directory structure.\nThis directory is added to the classpath when executing Pipelines.  The  vars  directory hosts scripts that define global variables accessible from\nPipeline scripts.\nThe basename of each  *.groovy  file should be a Groovy (~ Java) identifier, conventionally  camelCased .\nThe matching  *.txt , if present, can contain documentation, processed through the system\u2019s configured markup formatter\n(so may really be HTML, Markdown, etc., though the  txt  extension is required).  The Groovy source files in these directories get the same \u201cCPS transformation\u201d as your Pipeline scripts.  A  resources  directory allows the  libraryResource  step to be used from an external library to load associated non-Groovy files.\nCurrently this feature is not supported for internal libraries.  Other directories under the root are reserved for future enhancements.", 
            "title": "Library Directory structure"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/#configure-libraries-in-jenkins", 
            "text": "The a Jenkins Master you can configure the Global Pipeline Libraries.  You can find this in: Manage Jenkins -  Configure System -  Global Pipeline Libraries  You can configure multiple libraries, where the there is a preference for Git repositories.\nYou can select a default version (for example: the master branch), and either allow or disallow overrides to this.  To be able to use a different version, you would use the @  in case of Git.  @Library ( FlussoGlobal @my - feature - branch )", 
            "title": "Configure libraries in Jenkins"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/#helloworld-example", 
            "text": "Create Git repository (see below for structure)  Configure this Git repository as an \"Global Pipeline Libraries\" entry  Name: FlussoGlobal  Default Version: master  Modern SCM: git  Project repository:  :CICD/jenkins-pipeline-library.git  Create the resources you want in the git repository  Use the library in a pipeline", 
            "title": "HelloWorld Example"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/#util-class-class-example", 
            "text": "#!/usr/bin/groovy  # /src/ nl /flusso/ Utilities . groovy  package   nl . flusso  import   java.io.Serializable  class   Utilities   implements   Serializable   { \n   def   steps \n\n   Utilities ( steps )   { this . steps   =   steps } \n\n   def   sayHello ( String   name )   { \n     steps . sh   echo $name \n   }  }   @Library ( FlussoGlobal )  import   nl.flusso.Utilities  def   utils   =   new   Utilities ( steps )  node   { \n     String   name   =   Joost \n     utils . sayHello ( name )  }", 
            "title": "Util Class (class) Example"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/#util-method-var-example", 
            "text": "#!/usr/bin/groovy  # /vars/ sayHello . groovy  def   call ( name )   { \n     // you can call any valid step functions from your code, just like you can from Pipeline scripts \n     echo   Hello world, ${name}  }   @Library ( FlussoGlobal )   _  node   { \n     String   name   =   Joost \n     sayHello   name  }", 
            "title": "Util method (var) Example"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/#combining-libraries", 
            "text": "Lets say you want to want to have a core library and multiple specific libraries that utilize these. There are several to do this, we will show two.", 
            "title": "Combining libraries"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/#import-both", 
            "text": "One way is to explicitly import both libraries in the Jenkinsfile.  @Library ([ github.com/joostvdg/jenkins-pipeline-lib , github.com/joostvdg/jenkins-pipeline-go ])   _   Con:   you have to import all the required libraries yourself   Pro:   you can specify the versions of each", 
            "title": "Import both"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/#implicit-import-explicit-import", 
            "text": "You can also configure the core (in this case jenkins-pipeline-lib) as \"loaded implicitly\".\nThis will make anything from this library available by default.  Be careful with the naming of the vars though!  The resulting Jenkinsfile would then be.  @Library ( github.com/joostvdg/jenkins-pipeline-go )   _", 
            "title": "Implicit Import + Explicit Import"
        }, 
        {
            "location": "/jenkins-pipeline/global-shared-library/#resources", 
            "text": "implement-reusable-function-call", 
            "title": "Resources"
        }, 
        {
            "location": "/jenkins-pipeline/ide-integration-pipeline-dsl/", 
            "text": "IDE Integration for Jenkins Pipeline DSL\n\n\nSupported IDE's\n\n\nCurrently only Jetbrain's Intelli J's IDEA is \nsupported\n.\n\n\nThis via a Groovy DSL file (.gdsl).\n\n\nConfigure Intelli J IDEA\n\n\nGo to a Jenkins Pipeline job and open the Pipeline Syntax page.\n\n\nOn the page in the left hand menu, you will see a link to download a Jenkins Master specific Groovy DSL file.\nDownload this and save it into your project's workspace.\n\n\n\n\nIt will have to be part of your classpath, the easiest way to do this is to add the file as pipeline.gdsl in a/the src folder.\n\n\nFor more information, you can read \nSteffen Gerbert\n's blog.\n\n\nRemarks from Kohsuke Kawaguchi\n\n\nMore effort in this space will be taken by Cloudbees.\nBut the priority is low compared to other initiatives.\n\n\nIntegration of Pipeline Library\n\n\nIf you're using the Global Shared Libraries for sharing generic pipeline building blocks, it would be nice to have this awareness in your editor as well.\n\n\nOne of the ways to do this, is to checkout the source code of this library and make sure it is compiled.\nIn your editor (assuming Intelli J IDEA) you can then add the compiled classes as dependency (type: classes).\nThis way, at least every class defined in your library is usable as a normal dependency would be. \n\n\nFinal configuration Intelli J IDEA", 
            "title": "DSL IDE Integration"
        }, 
        {
            "location": "/jenkins-pipeline/ide-integration-pipeline-dsl/#ide-integration-for-jenkins-pipeline-dsl", 
            "text": "", 
            "title": "IDE Integration for Jenkins Pipeline DSL"
        }, 
        {
            "location": "/jenkins-pipeline/ide-integration-pipeline-dsl/#supported-ides", 
            "text": "Currently only Jetbrain's Intelli J's IDEA is  supported .  This via a Groovy DSL file (.gdsl).", 
            "title": "Supported IDE's"
        }, 
        {
            "location": "/jenkins-pipeline/ide-integration-pipeline-dsl/#configure-intelli-j-idea", 
            "text": "Go to a Jenkins Pipeline job and open the Pipeline Syntax page.  On the page in the left hand menu, you will see a link to download a Jenkins Master specific Groovy DSL file.\nDownload this and save it into your project's workspace.   It will have to be part of your classpath, the easiest way to do this is to add the file as pipeline.gdsl in a/the src folder.  For more information, you can read  Steffen Gerbert 's blog.", 
            "title": "Configure Intelli J IDEA"
        }, 
        {
            "location": "/jenkins-pipeline/ide-integration-pipeline-dsl/#remarks-from-kohsuke-kawaguchi", 
            "text": "More effort in this space will be taken by Cloudbees.\nBut the priority is low compared to other initiatives.", 
            "title": "Remarks from Kohsuke Kawaguchi"
        }, 
        {
            "location": "/jenkins-pipeline/ide-integration-pipeline-dsl/#integration-of-pipeline-library", 
            "text": "If you're using the Global Shared Libraries for sharing generic pipeline building blocks, it would be nice to have this awareness in your editor as well.  One of the ways to do this, is to checkout the source code of this library and make sure it is compiled.\nIn your editor (assuming Intelli J IDEA) you can then add the compiled classes as dependency (type: classes).\nThis way, at least every class defined in your library is usable as a normal dependency would be.", 
            "title": "Integration of Pipeline Library"
        }, 
        {
            "location": "/jenkins-pipeline/ide-integration-pipeline-dsl/#final-configuration-intelli-j-idea", 
            "text": "", 
            "title": "Final configuration Intelli J IDEA"
        }, 
        {
            "location": "/jenkins-pipeline/jenkins-parallel-pipeline/", 
            "text": "", 
            "title": "Parallel"
        }, 
        {
            "location": "/jenkins-pipeline/input/", 
            "text": "Jenkins Pipeline - Input\n\n\nThe Jenkins Pipeline has a plugin for dealing with external input.\nGenerally it is used to gather user input (values or approval), but it also has a REST API for this.\n\n\nGeneral Info\n\n\nThe \nPipeline Input Step\n allows you to\n\n\nThe plugin allows you to capture input in a variety of ways, but there are some gotcha's.\n\n\n\n\nIf you have a single parameter, it will be returned as a single value\n\n\nIf you have multiple parameters, it will be returned as a map\n\n\nThe choices for the Choice parameter should be a single line, where values are separated with /n\n\n\nDon't use input within a node {}, as this will block an executor slot\n\n\n..\n\n\n\n\nExamples\n\n\nSingle Parameter\n\n\ndef\n \nhello\n \n=\n \ninput\n \nid:\n \nCustomId\n,\n \nmessage:\n \nWant to continue?\n,\n \nok:\n \nYes\n,\n \nparameters:\n \n[\nstring\n(\ndefaultValue:\n \nworld\n,\n \ndescription:\n \n,\n \nname:\n \nhello\n)]\n\n\n\nnode\n \n{\n\n    \nprintln\n \necho $hello\n\n\n}\n\n\n\n\n\n\nMultiple Parameters\n\n\ndef\n \nuserInput\n \n=\n \ninput\n \nid:\n \nCustomId\n,\n \nmessage:\n \nWant to continue?\n,\n \nok:\n \nYes\n,\n \nparameters:\n \n[\nstring\n(\ndefaultValue:\n \nworld\n,\n \ndescription:\n \n,\n \nname:\n \nhello\n),\n \nstring\n(\ndefaultValue:\n \n,\n \ndescription:\n \n,\n \nname:\n \ntoken\n)]\n\n\n\nnode\n \n{\n\n    \ndef\n \nhello\n \n=\n \nuserInput\n[\nhello\n]\n\n    \ndef\n \ntoken\n \n=\n \nuserInput\n[\ntoken\n]\n\n    \nprintln\n \nhello=$hello, token=$token\n\n\n}\n\n\n\n\n\n\nTimeout on Input\n\n\ndef\n \nuserInput\n\n\n\ntimeout\n(\ntime:\n \n10\n,\n \nunit:\n \nSECONDS\n)\n \n{\n\n    \nprintln\n \nWaiting for input\n\n    \nuserInput\n \n=\n \ninput\n \nid:\n \nCustomId\n,\n \nmessage:\n \nWant to continue?\n,\n \nok:\n \nYes\n,\n \nparameters:\n \n[\nstring\n(\ndefaultValue:\n \nworld\n,\n \ndescription:\n \n,\n \nname:\n \nhello\n),\n \nstring\n(\ndefaultValue:\n \n,\n \ndescription:\n \n,\n \nname:\n \ntoken\n)]\n\n\n}\n\n\n\n\n\n\nREST API\n\n\nThere's a rest API for sending the input to a waiting input step.\nThe format of the url: \n\\({JenkinsURL}/\\)\n{JobURL}/\n\\({Build#}/input/\\)\n{InputID}/submit.\n\n\nThere are some things to keep in mind:\n\n\n\n\nIf Jenkins has CSRF protection enabled, you need a Crumb (see below) for the requests\n\n\nRequests are send via POST\n\n\nFor supplying values you need to have a JSON with the parameters with as \njson\n param\n\n\nYou need to supply the \nproceed\n value: the value of the \nok\n button, as \nproceed\n param\n\n\nYou will have to fill in the \ninput id\n, so it is best to configure a unique input id for the input steps you want to connect to from outside\n\n\n\n\nExamples\n\n\n{\nparameter\n:\n\n    \n[\n\n        \n{\nname\n:\n \nhello\n,\n \nvalue\n:\n \njoost\n},\n\n        \n{\nname\n:\n \ntoken\n,\n \nvalue\n:\n \nnot a token\n}\n\n    \n]\n\n\n}\n\n\n\n\n\n\n# single parameter\n\ncurl --user \n$USER\n:\n$PASS\n -X POST -H \nJenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33\n -d \njson\n=\n{\nparameter\n: {\nname\n: \nhello\n, \nvalue\n: \njoost\n}}\n -d \nproceed\n=\nYes\n \nhttps://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit\n\n\n\n\n\n# Multiple Parameters\n\ncurl --user \n$USER\n:\n$PASS\n -X POST -H \nJenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33\n -d \njson\n=\n{\nparameter\n: [{\nname\n: \nhello\n, \nvalue\n: \njoost\n},{\nname\n: \ntoken\n, \nvalue\n: \nnot a token\n}]}\n -d \nproceed\n=\nYes\n \nhttps://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit\n\n\n\n\n\n\nCrumb (secured Jenkins)\n\n\nIf Jenkins is secured against CSRF (via Global Security: Prevent Cross Site Request Forgery exploits), any API call requires a Crumb.\nYou can read more about it \nhere\n.\n\n\nTo get a valid crumb you have to send a crumb request as authenticated user.\n\n\n\n\nJSON: \nhttps://ci.flusso.nl/jenkins/crumbIssuer/api/json\n\n\nXML (parsed): \nhttps://ci.flusso.nl/jenkins/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,%22:%22,//crumb\n)", 
            "title": "Input"
        }, 
        {
            "location": "/jenkins-pipeline/input/#jenkins-pipeline-input", 
            "text": "The Jenkins Pipeline has a plugin for dealing with external input.\nGenerally it is used to gather user input (values or approval), but it also has a REST API for this.", 
            "title": "Jenkins Pipeline - Input"
        }, 
        {
            "location": "/jenkins-pipeline/input/#general-info", 
            "text": "The  Pipeline Input Step  allows you to  The plugin allows you to capture input in a variety of ways, but there are some gotcha's.   If you have a single parameter, it will be returned as a single value  If you have multiple parameters, it will be returned as a map  The choices for the Choice parameter should be a single line, where values are separated with /n  Don't use input within a node {}, as this will block an executor slot  ..", 
            "title": "General Info"
        }, 
        {
            "location": "/jenkins-pipeline/input/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/jenkins-pipeline/input/#single-parameter", 
            "text": "def   hello   =   input   id:   CustomId ,   message:   Want to continue? ,   ok:   Yes ,   parameters:   [ string ( defaultValue:   world ,   description:   ,   name:   hello )]  node   { \n     println   echo $hello  }", 
            "title": "Single Parameter"
        }, 
        {
            "location": "/jenkins-pipeline/input/#multiple-parameters", 
            "text": "def   userInput   =   input   id:   CustomId ,   message:   Want to continue? ,   ok:   Yes ,   parameters:   [ string ( defaultValue:   world ,   description:   ,   name:   hello ),   string ( defaultValue:   ,   description:   ,   name:   token )]  node   { \n     def   hello   =   userInput [ hello ] \n     def   token   =   userInput [ token ] \n     println   hello=$hello, token=$token  }", 
            "title": "Multiple Parameters"
        }, 
        {
            "location": "/jenkins-pipeline/input/#timeout-on-input", 
            "text": "def   userInput  timeout ( time:   10 ,   unit:   SECONDS )   { \n     println   Waiting for input \n     userInput   =   input   id:   CustomId ,   message:   Want to continue? ,   ok:   Yes ,   parameters:   [ string ( defaultValue:   world ,   description:   ,   name:   hello ),   string ( defaultValue:   ,   description:   ,   name:   token )]  }", 
            "title": "Timeout on Input"
        }, 
        {
            "location": "/jenkins-pipeline/input/#rest-api", 
            "text": "There's a rest API for sending the input to a waiting input step.\nThe format of the url:  \\({JenkinsURL}/\\) {JobURL}/ \\({Build#}/input/\\) {InputID}/submit.  There are some things to keep in mind:   If Jenkins has CSRF protection enabled, you need a Crumb (see below) for the requests  Requests are send via POST  For supplying values you need to have a JSON with the parameters with as  json  param  You need to supply the  proceed  value: the value of the  ok  button, as  proceed  param  You will have to fill in the  input id , so it is best to configure a unique input id for the input steps you want to connect to from outside", 
            "title": "REST API"
        }, 
        {
            "location": "/jenkins-pipeline/input/#examples_1", 
            "text": "{ parameter : \n     [ \n         { name :   hello ,   value :   joost }, \n         { name :   token ,   value :   not a token } \n     ]  }   # single parameter \ncurl --user  $USER : $PASS  -X POST -H  Jenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33  -d  json = { parameter : { name :  hello ,  value :  joost }}  -d  proceed = Yes   https://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit   # Multiple Parameters \ncurl --user  $USER : $PASS  -X POST -H  Jenkins-Crumb:b220147dbdf3cfebbeba4c29048c2e33  -d  json = { parameter : [{ name :  hello ,  value :  joost },{ name :  token ,  value :  not a token }]}  -d  proceed = Yes   https://ci.flusso.nl/jenkins/job/Joost/job/Pipeline-Example/5/input/CustomId/submit", 
            "title": "Examples"
        }, 
        {
            "location": "/jenkins-pipeline/input/#crumb-secured-jenkins", 
            "text": "If Jenkins is secured against CSRF (via Global Security: Prevent Cross Site Request Forgery exploits), any API call requires a Crumb.\nYou can read more about it  here .  To get a valid crumb you have to send a crumb request as authenticated user.   JSON:  https://ci.flusso.nl/jenkins/crumbIssuer/api/json  XML (parsed):  https://ci.flusso.nl/jenkins/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,%22:%22,//crumb )", 
            "title": "Crumb (secured Jenkins)"
        }
    ]
}